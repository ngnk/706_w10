---
# Airflow Taxi Data Pipeline
---
## Project Overview

This project uses Apache Airflow to implement an ETL pipeline for NYC taxi data.

The pipeline performs the following actions:
1.  **Ingest:** Locates Parquet files for "Yellow" and "Green" taxi trips from January 2025.
2.  **Transform:** Cleans, standardizes, and enriches the data from both files in parallel using `pandas`. This includes calculating trip duration, standardizing columns, and calculating a tip rate.
3.  **Load:** Merges the two cleaned datasets and loads the combined data into a PostgreSQL data warehouse. It populates two tables:
    * `warehouse.taxi_trips_clean`: A detailed table of all individual trips.
    * `warehouse.zone_daily_agg`: A daily aggregation of trips, average distance, and tip rate per pickup location.
4.  **Analyze:** After the data is loaded, two parallel analysis tasks run:
    * A chart (`top_zones.png`) is generated using `matplotlib` to show the top 15 most popular pickup zones.
    * A simple machine learning report (`ml_report.txt`) is generated by training a `scikit-learn` linear regression model to predict `tip_rate` based on `trip_distance` and `trip_minutes`.

---

## Setup Instructions

### Prerequisites

* Docker - Ensure it is open and running while executing the workflow.
* Datasets - Download [Here](https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page). I slashed the datasets to reduce size, so use the ones provided within the repository.

### Setup

1. **Clone this repository to your desired directory.**
2. **Ensure the docker daemon is running.**

3.  **Build and Start the Services**

    From the project's root directory, run the following command to build the custom Airflow image and start all services (Postgres, Airflow Webserver, Scheduler) in detached mode:

    ```bash
    docker compose up airflow-init
    docker compose up -d
    ```
    
    Wait a minute or two for all services to initialize. 

5.  **Access the Airflow UI**

    Open your web browser and go to: **http://localhost:8080**

6.  **Log In**

    The `airflow-init` service creates a default user for you.
    * **Username:** `admin`
    * **Password:** `admin`

7.  **Run the Pipeline**

    1.  On the Airflow homepage, you will see the DAG named `week10_taxi_pipeline`.
    2.  By default, the DAG is paused. Click the **toggle switch** on the left to unpause it (make it active).
    3.  To run the pipeline manually, click the "Play" (Trigger DAG) button on the right.
    4.  You can click on the DAG name (`week10_taxi_pipeline`) to monitor the progress of the run in the Grid View.

---

## ðŸ“Š Accessing the Results

There are two ways to access the output of the pipeline.

### 1. File Outputs (Plots and Reports)

The analysis tasks save their results to the `./output` directory, which is mapped from the container.

After a successful DAG run, you can find the following files on your **local machine**:
* `./output/top_zones.png`
* `./output/ml_report.txt`

If you re-run the DAG, these files will be overwritten.

### 2. Data Warehouse (PostgreSQL)

The cleaned data and aggregations are stored in the PostgreSQL database. You can connect to it using any database client (like DBeaver, TablePlus, or `psql`).

Use the following connection details:
* **Host:** `localhost`
* **Port:** `5440` (Note: this is mapped to the container's port 5432)
* **Database:** `airflow`
* **User:** `airflow`
* **Password:** `airflow`

Once connected, you can query the tables in the `warehouse` schema:

```sql
SELECT * FROM warehouse.taxi_trips_clean LIMIT 10;

SELECT * FROM warehouse.zone_daily_agg ORDER BY trips DESC LIMIT 10;
```

---
