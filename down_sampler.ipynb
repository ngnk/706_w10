{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ec87cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Parquet downsampler: keep ~10% rows, write ZSTD-compressed output --- #\n",
    "# 1) pip install polars\n",
    "# 2) Edit INPUT_FILES if needed, then run.\n",
    "\n",
    "from pathlib import Path\n",
    "import os, math, time\n",
    "from typing import Sequence, Optional\n",
    "\n",
    "try:\n",
    "    import polars as pl\n",
    "except ImportError as e:\n",
    "    raise SystemExit(\n",
    "        \"Polars is required. Install via `pip install polars` and re-run this cell.\"\n",
    "    ) from e\n",
    "\n",
    "# --------------------------- CONFIG --------------------------------------- #\n",
    "# Files to process (edit this to your paths)\n",
    "INPUT_FILES: Sequence[str] = [\n",
    "    \"\",\n",
    "    \"\",\n",
    "]\n",
    "\n",
    "KEEP_FRACTION: float = 0.10     # keep ~10% of rows (i.e., remove ~90%)\n",
    "SEED: int = 42                  # for reproducible sampling\n",
    "COMPRESSION: str = \"zstd\"       # 'zstd' for best compression, 'snappy' is faster but larger\n",
    "COMPRESSION_LEVEL: int = 7      # 1-22 for zstd; 5-9 are good tradeoffs\n",
    "WRITE_STATISTICS: bool = True   # write column stats into Parquet metadata\n",
    "# Optional: preserve class balance by sampling within groups (e.g., \"VendorID\")\n",
    "STRATIFY_BY: Optional[str] = None    # e.g., \"VendorID\" or None\n",
    "# ------------------------------------------------------------------------- #\n",
    "\n",
    "def _fmt_mb(bytes_: int) -> float:\n",
    "    return round(bytes_ / (1024 * 1024), 2)\n",
    "\n",
    "def _out_path(p: Path, keep_fraction: float, compression: str) -> Path:\n",
    "    stem = p.stem\n",
    "    parent = p.parent\n",
    "    suffix = p.suffix  # .parquet\n",
    "    pct = int(keep_fraction * 100)\n",
    "    return parent / f\"{stem}_sample{pct}_{compression}.parquet\"\n",
    "\n",
    "def downsample_parquet(\n",
    "    in_path: str,\n",
    "    keep_fraction: float = KEEP_FRACTION,\n",
    "    seed: int = SEED,\n",
    "    compression: str = COMPRESSION,\n",
    "    compression_level: int = COMPRESSION_LEVEL,\n",
    "    write_statistics: bool = WRITE_STATISTICS,\n",
    "    stratify_by: Optional[str] = STRATIFY_BY,\n",
    "):\n",
    "    p = Path(in_path)\n",
    "    if not p.exists():\n",
    "        return {\"file\": p.name, \"status\": \"ERROR\", \"message\": \"File not found.\"}\n",
    "\n",
    "    start = time.time()\n",
    "    orig_size = p.stat().st_size\n",
    "\n",
    "    # Read entire file (quick for typical monthly NYC Taxi parquet files)\n",
    "    df = pl.read_parquet(str(p))\n",
    "    original_rows = df.height\n",
    "    if original_rows == 0:\n",
    "        out_path = _out_path(p, keep_fraction, compression)\n",
    "        # write empty frame preserving schema\n",
    "        df.write_parquet(str(out_path), compression=compression, compression_level=compression_level, statistics=write_statistics)\n",
    "        new_size = Path(out_path).stat().st_size\n",
    "        return {\n",
    "            \"file\": p.name,\n",
    "            \"original_rows\": 0,\n",
    "            \"new_rows\": 0,\n",
    "            \"kept_fraction\": 0.0,\n",
    "            \"original_size_mb\": _fmt_mb(orig_size),\n",
    "            \"new_size_mb\": _fmt_mb(new_size),\n",
    "            \"size_reduction_pct\": 0.0,\n",
    "            \"output_file\": str(out_path),\n",
    "            \"status\": \"OK\",\n",
    "            \"elapsed_s\": round(time.time() - start, 2),\n",
    "        }\n",
    "\n",
    "    # Sample ~10% without replacement, reproducible\n",
    "    if stratify_by and stratify_by in df.columns:\n",
    "        df_small = df.sample(\n",
    "            fraction=keep_fraction,\n",
    "            with_replacement=False,\n",
    "            shuffle=True,\n",
    "            seed=seed,\n",
    "            stratify_by=stratify_by,\n",
    "        )\n",
    "    else:\n",
    "        df_small = df.sample(\n",
    "            fraction=keep_fraction,\n",
    "            with_replacement=False,\n",
    "            shuffle=True,\n",
    "            seed=seed,\n",
    "        )\n",
    "\n",
    "    new_rows = df_small.height\n",
    "    out_path = _out_path(p, keep_fraction, compression)\n",
    "\n",
    "    # Write with ZSTD compression\n",
    "    df_small.write_parquet(\n",
    "        str(out_path),\n",
    "        compression=compression,\n",
    "        compression_level=compression_level,\n",
    "        statistics=write_statistics,\n",
    "    )\n",
    "    new_size = Path(out_path).stat().st_size\n",
    "    reduction_pct = 100.0 * (orig_size - new_size) / orig_size if orig_size > 0 else 0.0\n",
    "\n",
    "    return {\n",
    "        \"file\": p.name,\n",
    "        \"original_rows\": original_rows,\n",
    "        \"new_rows\": new_rows,\n",
    "        \"kept_fraction\": round(new_rows / original_rows, 4),\n",
    "        \"original_size_mb\": _fmt_mb(orig_size),\n",
    "        \"new_size_mb\": _fmt_mb(new_size),\n",
    "        \"size_reduction_pct\": round(reduction_pct, 2),\n",
    "        \"output_file\": str(out_path),\n",
    "        \"status\": \"OK\",\n",
    "        \"elapsed_s\": round(time.time() - start, 2),\n",
    "    }\n",
    "\n",
    "# --------------------------- RUN ------------------------------------------ #\n",
    "results = []\n",
    "for f in INPUT_FILES:\n",
    "    try:\n",
    "        res = downsample_parquet(f)\n",
    "    except Exception as e:\n",
    "        res = {\"file\": f, \"status\": \"ERROR\", \"message\": str(e)}\n",
    "    results.append(res)\n",
    "\n",
    "# Pretty print a compact summary\n",
    "try:\n",
    "    import pandas as pd\n",
    "    display(pd.DataFrame(results))\n",
    "except Exception:\n",
    "    # fallback plain print\n",
    "    for r in results:\n",
    "        print(r)\n",
    "\n",
    "# Notes:\n",
    "# - To target a *size* reduction precisely (e.g., 90% smaller file bytes),\n",
    "#   start with KEEP_FRACTION=0.10. If the output is still too big, drop to 0.08 or 0.05,\n",
    "#   then re-run. Size depends on column entropy and compression.\n",
    "# - For exact class balancing, set STRATIFY_BY=\"your_column\".\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
